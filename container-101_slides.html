<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="June 10, 2020" />
  <title>Container 101 on Lawrencium</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Container 101 on Lawrencium</h1>
  <p class="author">
June 10, 2020
  </p>
  <p class="date">Wei Feinstein</p>
</div>
<div id="outline" class="slide section level1">
<h1>Outline</h1>
<ul>
<li>Lawrencium Supercluster overview</li>
<li>Computing resources &amp; services<br />
</li>
<li>Container technology overview</li>
<li>Build singularity containers</li>
<li>Run singularity containers on Lawrencium</li>
</ul>
</div>
<div id="lawrencium-cluster-overview" class="slide section level1">
<h1>Lawrencium Cluster Overview</h1>
<center>
<img src="figures/lrc.png" width="80%">
</center>
</div>
<div id="lawrencium-condo-cluster" class="slide section level1">
<h1>Lawrencium Condo Cluster</h1>
<ul>
<li>Lawrencium is a LBNL Condo Computer clusterresources
<ul>
<li>LBNL investmeent</li>
<li>Individual PIs purchase nodes and storage</li>
<li>Computational cycles are shared among all lawrencium users</li>
</ul></li>
<li>Cluster infrastructure:
<ul>
<li>OTP authentication</li>
<li>High speed infiniBand parallel file system for fast inter-node communication</li>
<li>OS and security updates, software module farm, job scheduler SLURM</li>
<li>Home/project storage, lustre parallel file system and backend network infrastructure.</li>
</ul></li>
</ul>
</div>
<div id="accounts-on-lawrencium" class="slide section level1">
<h1>Accounts on Lawrencium</h1>
<h2 id="three-types-of-project-accounts">Three types of Project Accounts</h2>
<ul>
<li><p>PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx)</p></li>
<li><p>Condo account:</p>
<ul>
<li>PIs purchase and contribute compute nodes to the general condo pool (lr_xxx)</li>
<li>Jobs are free of charge and have highest priority</li>
</ul></li>
<li><p>Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx)</p></li>
<li><p>$25 per user per month</p></li>
</ul>
<p>https://sites.google.com/a/lbl.gov/hpc/getting-an-account</p>
<h2 id="user-accounts">User accounts</h2>
<ul>
<li>User account request</li>
<li>User agreement consent</li>
</ul>
<p>https://sites.google.com/a/lbl.gov/hpc/getting-an-account</p>
</div>
<div id="softwre-module-farm" class="slide section level1">
<h1>Softwre Module Farm</h1>
<center>
<img src="figures/SMF.png" width="60%">
</center>
</div>
<div id="module-commands" class="slide section level1">
<h1>Module commands</h1>
<ul>
<li><em>module purge</em>: clear user’s work environment</li>
<li><em>module avail</em>: check available software packages</li>
<li><em>module load xxx</em>: load a package</li>
<li><em>module list</em>: check currently loaded software</li>
<li>Users may install their own software</li>
</ul>
<p>https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide</p>
</div>
<div id="slurm" class="slide section level1">
<h1>SLURM:</h1>
<h2 id="jub-submission">Jub Submission</h2>
<ul>
<li>sbatch: submit a job to the batch queue system</li>
</ul>
<pre><code>sbatch myjob.sh</code></pre>
<ul>
<li>srun: request an interactive node(s) and login automatically</li>
</ul>
<pre><code>srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash</code></pre>
<ul>
<li>salloc : request an interactive node(s)</li>
</ul>
<pre><code>salloc –A pc_xxx –p lr6 –q lr_debug –t 0:30:0</code></pre>
<h2 id="job-monitoring">Job Monitoring</h2>
<ul>
<li>sinfo: check status of partitions and nodes (idle, allocated, drain, down)</li>
</ul>
<pre><code>sinfo –r –p lr6</code></pre>
<ul>
<li>squeue: check jobs in the batch queuing system (R or PD)</li>
</ul>
<pre><code>squeue –u $USER</code></pre>
<ul>
<li>sacct: check job information or history</li>
</ul>
<pre><code>sacct -X -o ‘jobid,user,partition,nodelist,stat’</code></pre>
<ul>
<li>scancel : cancel a job</li>
</ul>
<pre><code>scancel jobID</code></pre>
<p>https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions</p>
</div>
<div id="services-1" class="slide section level1">
<h1>Services (1)</h1>
<ul>
<li>Designate Data Transfer node <strong>lrc-xfer.lbl.gov</strong>
<ul>
<li>scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path</li>
<li>rsync -avzh /your/source/file $USER <span class="citation">@lrc-xfer.lbl.gov:/cluster/path</span></li>
</ul></li>
<li>Globus Online provide secured unified interface for data transfer
<ul>
<li>endpoint lbn#lrc, Globus Connect, AWS S3 connect</li>
</ul></li>
<li>Visualization and remote desktop
<ul>
<li>Detailed information <a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop">click here</a></li>
</ul></li>
</ul>
</div>
<div id="service-2-jupyterhub" class="slide section level1">
<h1>Service (2): Jupyterhub</h1>
<p>https://lrc-jupyter.lbl.gov</p>
<center>
<img src="figures/jupyter.png" width="80%">
</center>
</div>
<div id="services-3-cloud-computing" class="slide section level1">
<h1>Services (3): Cloud Computing</h1>
<ul>
<li>LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP).<br />
</li>
<li>No charge to have an account in the program</li>
<li>Charges only for actual usage of cloud services like storage or compute</li>
<li>Monthly billing for cloud usage are direct to your PID via recharge mechanism</li>
<li>Simple enrollment process.</li>
<li>De-enrollment only changes the billing setup in your account, and your account will continue to be active.</li>
<li>Complete control of your own AWS or GCP dashboard, your data, tools, and services.</li>
</ul>
</div>
<div id="servicesv-4-cloud-computing" class="slide section level1">
<h1>Servicesv (4): Cloud Computing</h1>
<ul>
<li>AWS and GCP make discounts available to LBNL users.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Type</th>
<th>AWS</th>
<th>GCP</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Overall</td>
<td>7%</td>
<td>13%</td>
<td></td>
</tr>
<tr class="even">
<td>Data Egress</td>
<td>15%</td>
<td>25%</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>AWS, <a href="https://aws.amazon.com/blogs/publicsector/aws-offers-data-egress-discount-to-researchers/">restrictions apply</a> |</li>
<li>Over 125 people at LBNL with cloud accounts on AWS and GCP.<br />
</li>
<li>Mostly use virtual machines and storage</li>
<li>Use containerization, machine learning, AI, and data visualization services on both platforms</li>
<li>To set up a cloud account on either AWS or GCP, send email to <a href="mailto:scienceit@lbl.gov">scienceit@lbl.gov</a></li>
</ul>
</div>
<div id="containerization" class="slide section level1">
<h1>Containerization</h1>
<ul>
<li>Technology of putting an application and all of its dependencies into a single package.</li>
<li>Portable, shareable, and reproducible.</li>
<li>Your application brings its environment with it.</li>
</ul>
</div>
<div id="containerization-examples" class="slide section level1">
<h1>Containerization Examples</h1>
<ul>
<li>Package an analysis pipeline so that it runs on your laptop, in the cloud, and in a high performance computing (HPC) environment to produce the same result.</li>
<li>Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results.</li>
<li>Install and run an application that requires a complicated stack of dependencies with a few keystrokes.</li>
<li>Create a pipeline or complex workflow where each individual program is meant to run on a different operating system.</li>
</ul>
</div>
<div id="container-vs.-virtual-machine" class="slide section level1">
<h1>Container vs. Virtual Machine</h1>
<center>
<img src="figures/vm-sif.png" width="70%">
</center>
</div>
<div id="vm-services" class="slide section level1">
<h1>VM services</h1>
<center>
<img src="figures/vm.png" width="50%">
</center>
<p><a href="https://commons.lbl.gov/display/itfaq/SVM+-+Virtual+Machine+Hosting">More information about VM</a></p>
</div>
<div id="singularity-technology" class="slide section level1">
<h1>Singularity Technology</h1>
<ul>
<li>Open-source computer software that encapsulates an application and all its dependencies into a single image</li>
<li>Bring containers and reproducibility to scientific computing and HPC</li>
<li>Developed by Greg Kurtzer</li>
<li>Singularity assumes that you will have a build system where you are the root user, but that you will also have a production system where you may or may not be the root user.</li>
</ul>
</div>
<div id="singularity-workflow" class="slide section level1">
<h1>Singularity Workflow</h1>
<ul>
<li>Install Singularity on a local machine</li>
<li>Build Singularity images/containers on the local machine</li>
<li>Transfer images/containers to LRC clusters</li>
<li>Run images /containers on the cluster
<ul>
<li>Root privilege is not permitted</li>
</ul></li>
</ul>
</div>
<div id="install-singularity" class="slide section level1">
<h1>Install Singularity</h1>
<p>Three OS platforms: - Linux - Mac - Window Installation <a href="https://github.com/lbnl-science-it/container-101/blob/master/singularity_installation_guide.md">instructions</a></p>
<pre><code>$ singularity --version 
$ singularity run docker://godlovedc/lolcow</code></pre>
</div>
<div id="create-singularity-containers" class="slide section level1">
<h1>Create Singularity Containers</h1>
<ul>
<li>Directly from existing containers
<ul>
<li><a href="https://hub.docker.com/search?q=&amp;type=image">Docker hub</a></li>
<li><a href="https://cloud.sylabs.io/library">Sylabs Cloud</a> and <a href="https://singularity-hub.org/">Singularity hub</a></li>
<li><a href="https://www.nvidia.com/en-us/gpu-cloud/containers/">Nvidia HPC containers</a></li>
<li><a href="https://biocontainers.pro/#/registry">Biocontainers</a></li>
<li><a href="https://aws.amazon.com/releasenotes/available-deep-learning-containers-images/">AWS</a></li>
</ul></li>
<li>Build from definition files or recipes</li>
</ul>
</div>
<div id="singularity-pull" class="slide section level1">
<h1>Singularity pull</h1>
<ul>
<li>No root/sudo privilege is needed</li>
<li>Create immutable squashfs containers</li>
</ul>
<pre><code>singularity pull --help</code></pre>
<ul>
<li>Docker Hub:  Pull a container from Docker Hub.</li>
</ul>
<pre><code>singularity pull docker://ubuntu:18.04 
singularity pull docker://gcc:7.2.0</code></pre>
<ul>
<li>Singularity Hub:  If no tag is specified, the master branch of the repository is used</li>
</ul>
<pre><code>singularity pull shub://singularityhub/hello-world</code></pre>
</div>
<div id="singularity-shell-run-exec" class="slide section level1">
<h1>Singularity shell, run, exec</h1>
<ul>
<li><strong>shell</strong> sub-command: invokes an interactive shell within a container</li>
</ul>
<pre><code>singularity shell hello-world_latest.sif</code></pre>
<ul>
<li><strong>run</strong> sub-command: executes the container’s runscript</li>
</ul>
<pre><code>singularity run hello-world_latest.sif </code></pre>
<ul>
<li><strong>exec</strong> sub-command: execute an arbitrary command within container</li>
</ul>
<pre><code>singularity exec hello-world_latest.sif cat /etc/os-release</code></pre>
</div>
<div id="singularity-build" class="slide section level1">
<h1>Singularity build</h1>
<ul>
<li>Root/sudo privilege is needed</li>
</ul>
<pre><code>singularity build --help</code></pre>
<ul>
<li>Build from a definition file</li>
</ul>
<pre><code>sudo singularity --debug build mycontainer.sif Singularity </code></pre>
</div>
<div id="defination-filerecipe" class="slide section level1">
<h1>Defination File/Recipe</h1>
<pre><code>Bootstrap: docker
From: ubuntu
# used singularity run-help 
%help
Hello. I&#39;m in the container.
# executed on host after the base OS is installed.
%setup
    touch ${SINGULARITY_ROOTFS}/tacos.txt
    echo “I love avocado” &gt;&gt; avocados.txt

# copy files from your host system into the container 
%files
    avocados.txt /opt    

%environment
  export NAME=avocado

# executed within the container after the base OS is installed at build time
#install new software and libraries, config files,  directories, etc
%post
    echo &#39;export Avocado=TRUE &gt;&gt; $SINGULARITY_ENVIRONMENT
# executed when the container image is run:  singularity run
%runscript 
    echo &quot;Hello! Arguments received: $* \n&quot;
     exec echo &quot;$@&quot;  </code></pre>
</div>
<div id="bootstrap-agents" class="slide section level1">
<h1>Bootstrap Agents</h1>
<ul>
<li>library (images hosted on the Container Library)</li>
<li>docker (images hosted on Docker Hub)</li>
<li>shub (images hosted on Singularity Hub)</li>
<li>localimage (images saved on your machine)</li>
<li>yum (yum based systems such as CentOS and Scientific Linux)</li>
<li>debootstrap (apt based systems such as Debian and Ubuntu)</li>
<li>arch (Arch Linux)</li>
<li>busybox (BusyBox)</li>
<li>zypper (zypper ba</li>
</ul>
</div>
<div id="singularity-build-rewritable-sandbox" class="slide section level1">
<h1>Singularity Build Rewritable Sandbox</h1>
<ul>
<li>Can be built from a recipe or existing container</li>
<li>Used to develop, test, and make changes, then build or convert it into a standard image</li>
</ul>
<pre><code>sudo singularity build --sandbox build gccbox docker://gcc:7.2.0
sudo singularity build --sandbox build test-box Singularity </code></pre>
<ul>
<li>When you want to alter your image, you can use commands like shell, exec, run, with the –writable option</li>
</ul>
<pre><code>sudo singularity shell --writable test-box</code></pre>
<ul>
<li>Convert a sandbox to an immutable final container:</li>
</ul>
<pre><code>sudo singularity build test-box.sif test-box</code></pre>
</div>
<div id="inspect-containers" class="slide section level1">
<h1>Inspect containers</h1>
<ul>
<li>To check how a images is built, running script and environment variables..</li>
</ul>
<pre><code>singularity inspect [options] image_name
    --lablels
    --runscript
    --deffile
    --environment
e.g. singularity inspect --deffile mycontainer.sif</code></pre>
</div>
<div id="singularity-python-spython" class="slide section level1">
<h1>Singularity Python (spython)</h1>
<ul>
<li>Python API for Singularity containers</li>
<li>Convert Dockerfile to Singularity def</li>
</ul>
<pre><code>spython recipe Dockerfile &gt; dock2sif.def</code></pre>
</div>
<div id="run-singularity-containers-on-lawrencium" class="slide section level1">
<h1>Run Singularity Containers on Lawrencium</h1>
<ul>
<li>File transfer to LRC cluster</li>
</ul>
<pre><code>scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster </code></pre>
<ul>
<li>Run your container interactively
<ul>
<li>Request an interactive compute node</li>
</ul>
<pre><code>singularity shell/run/exec container.sif</code></pre></li>
<li>Submit a slurm job</li>
</ul>
</div>
<div id="job-submission-example" class="slide section level1">
<h1>Job Submission Example</h1>
<pre><code>#!/bin/bash -l
#SBATCH --job-name=container-test        
#SBATCH --partition=lr5          
#SBATCH --account=ac_xxx         
#SBATCH --qos=lr_normal         
#SBATCH --nodes=1           
#SBATCH --time=1-2:0:0          

cd $SLURM_SUBMIT_DIR
singularity exec mycontainer.sif cat /etc/os-release</code></pre>
</div>
<div id="container-bind-path" class="slide section level1">
<h1>Container bind path</h1>
<ul>
<li>Singularity allow mapping directories on host to directories within container</li>
<li>Easy data access within containers</li>
<li>System-defined bind paths on LRC
<ul>
<li>/global/home/users/</li>
<li>/globa/scratch/</li>
</ul></li>
<li>User can define own bind paths:</li>
<li>e.g.: mount /host/path/ on the host to /container/path inside the container</li>
</ul>
<pre><code>-B /host/path/:/container/path
singularity shell --nv -B /clusterfs/bear:/tmp pytorch_19_12_py3.sif</code></pre>
</div>
<div id="run-gpu-containers" class="slide section level1">
<h1>Run GPU Containers</h1>
<ul>
<li>Singularity supports NVIDIA’s CUDA GPU compute framework or AMD’s ROCm solution</li>
<li>–nv enables NVIDIA GPU support in Singularity</li>
<li>Remember to request a GPU node from the ES1 partition</li>
</ul>
<pre><code>singularity exec --nv pytorch_19_12_py3.sif python -c &quot;import torch; print(torch.__version__)&quot;
1.4.0a0+a5b4d78</code></pre>
</div>
<div id="getting-help" class="slide section level1">
<h1>Getting help</h1>
<ul>
<li>Virtual Office Hours:
<ul>
<li>Time: 10:30am - noon (Wednesdays)</li>
<li>Request online</li>
</ul></li>
<li>Sending us tickets at hpcshelp@lbl.gov</li>
<li>More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/</li>
<li>DLab consulting: https://dlab.berkeley.edu/consulting</li>
</ul>
</div>
</body>
</html>
