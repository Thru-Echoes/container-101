<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="June 10, 2020" />
  <title>Container 101 on Lawrencium</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Container 101 on Lawrencium</h1>
<p class="author">June 10, 2020</p>
<p class="date">Wei Feinstein</p>
</header>
<h1 id="outline">Outline</h1>
<ul>
<li>Lawrencium Supercluster overview</li>
<li>Computing resources &amp; services<br />
</li>
<li>Container technology overview</li>
<li>Build singularity containers</li>
<li>Run singularity containers on Lawrencium</li>
</ul>
<h1 id="lawrencium-cluster-overview">Lawrencium Cluster Overview</h1>
<center>
<img src="figures/lrc.png" width="80%">
</center>
<h1 id="lawrencium-condo-cluster">Lawrencium Condo Cluster</h1>
<ul>
<li>Lawrencium is a LBNL Condo Computer clusterresources
<ul>
<li>Significant investment from LBNL</li>
<li>Individual PIs purchase nodes and storage</li>
<li>Computational cycles are shared among all lawrencium users</li>
</ul></li>
<li>Cluster infrastructure:
<ul>
<li>OTP authentication</li>
<li>High speed infiniBand parallel file system for fast inter-node communication</li>
<li>OS and security updates, software module farm, job scheduler SLURM</li>
<li>Home/project storage, lustre parallel file system and backend network infrastructure.</li>
</ul></li>
</ul>
<h1 id="three-types-of-project-accounts">Three types of Project Accounts</h1>
<ul>
<li>PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx)</li>
<li>Condo account:
<ul>
<li>PIs purchase and contribute compute nodes to the general condo pool (lr_xxx)</li>
<li>Run jobs within their condo contributions for free</li>
</ul></li>
<li>Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx)</li>
<li>$25 per user per month</li>
<li>Refer to detailed information at <a href="https://sites.google.com/a/lbl.gov/hpc/getting-an-account">here</a></li>
</ul>
<h2 id="user-accounts">User accounts</h2>
<ul>
<li>User account request</li>
<li>User agreement consent</li>
<li>Check <a href="https://sites.google.com/a/lbl.gov/hpc/getting-an-account">here</a> for details</li>
</ul>
<h1 id="softwre-module-farm">Softwre Module Farm</h1>
<center>
<img src="figures/SMF.png" width="60%">
</center>
<h1 id="module-commands">Module commands</h1>
<ul>
<li><em>module purge</em>: clear user’s work environment</li>
<li>*module availablev: check available software packages</li>
<li><em>module load xxx</em>: load a package</li>
<li><em>module list</em>: check currently loaded software</li>
<li>Users may install their own software</li>
<li>More <a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide">information</a></li>
</ul>
<h1 id="jub-submission">Jub Submission</h1>
<ul>
<li>sbatch: submit a job to the batch queue system</li>
</ul>
<pre><code>sbatch myjob.sh</code></pre>
<ul>
<li>srun: request an interactive node(s) and login automatically</li>
</ul>
<pre><code>srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash</code></pre>
<ul>
<li>salloc : request an interactive node(s)</li>
</ul>
<pre><code>salloc –A pc_xxx –p lr6 –q lr_debug –t 0:30:0</code></pre>
<h1 id="job-monitoring">Job Monitoring</h1>
<ul>
<li>sinfo:view information about partitions and nodes (idle, allocated, drain, down )</li>
</ul>
<pre><code>sinfo –r –p lr6</code></pre>
<ul>
<li>squeue: check the current jobs in the batch queue system</li>
</ul>
<pre><code>squeue –u $USER</code></pre>
<ul>
<li>sacct: information on jobs</li>
</ul>
<pre><code>sacct -X -o ‘jobid,user,partition,nodelist,stat’</code></pre>
<ul>
<li>scancel : cancel a job</li>
</ul>
<pre><code>scancel jobID</code></pre>
<ul>
<li>More <a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions">information</a></li>
</ul>
<h1 id="resources">Resources</h1>
<ul>
<li>Data Transfer node lrc-xfer.lbl.gov
<ul>
<li>scp -r your/source/file $USER@lrc-xder.lbl.gov:/cluster/path
<ul>
<li>rsync -avzh your/source/file $USER <span class="citation" data-cites="lrc-xfer.lbl.gov:/cluster/path">@lrc-xfer.lbl.gov:/cluster/path</span></li>
</ul></li>
</ul></li>
<li>Globus Online provide secured unified interface for data transfer
<ul>
<li>endpoint lbn#lrc, Globus Connect, AWS S3 connect</li>
</ul></li>
<li>Visualization and remote desktop
<ul>
<li>Detailed <a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop">information</a></li>
</ul></li>
</ul>
<h1 id="service-jupyterhub">Service: Jupyterhub</h1>
<center>
<img src="figures/jupyter.png" width="80%">
</center>
<p>https://lrc-jupyter.lbl.gov</p>
<h1 id="services-cloud-computing">Services: Cloud Computing</h1>
<ul>
<li>LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP).<br />
</li>
<li><ul>
<li>No charge to have an account in the program</li>
</ul></li>
<li><ul>
<li>Charges only for actual usage of cloud services like storage or compute</li>
</ul></li>
<li><ul>
<li>Monthly billing for cloud usage are direct to your PID via recharge mechanism</li>
</ul></li>
<li><ul>
<li>Simple enrollment process.</li>
</ul></li>
<li><ul>
<li>De-enrollment only changes the billing setup in your account, and your account will continue to be active.</li>
</ul></li>
<li><ul>
<li>You maintain complete control of your own AWS or GCP dashboard, and your data, tools, and services in your account are only accessible by you.</li>
</ul></li>
</ul>
<h1 id="services-cloud-computing-1">Services: Cloud Computing</h1>
<ul>
<li><p>Both AWS and GCP make discounts available to LBNL users for using their services. Generally speaking, costs for using GCP will be lower than AWS due to lower pricing and additional discounts. The discounts are: | Type | AWS | GCP | | | — | — | — | — | | Overall | 7% | 13% | | | Data Egress | 15% | 25% |</p></li>
<li><p>AWS, <a href="https://aws.amazon.com/blogs/publicsector/aws-offers-data-egress-discount-to-researchers/">restrictions apply</a> |</p></li>
<li><p>Over 125 people at LBNL with cloud accounts on AWS and GCP.<br />
</p></li>
<li><p>Mostly use virtual machines and storage</p></li>
<li><p>Use containerization, machine learning, AI, and data visualization services on both platforms.</p></li>
<li><p>To set up a cloud account on either AWS or GCP, send email to <a href="mailto:scienceit@lbl.gov">scienceit@lbl.gov</a></p></li>
</ul>
<h1 id="containerization">Containerization</h1>
<ul>
<li>Technology of putting an application and all of its dependencies into a single package.</li>
<li>Portable, shareable, and reproducible.</li>
<li>Your application brings its environment with it.</li>
</ul>
<h1 id="containerization-examples">Containerization Examples</h1>
<ul>
<li>Package an analysis pipeline so that it runs on your laptop, in the cloud, and in a high performance computing (HPC) environment to produce the same result.</li>
<li>Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results.</li>
<li>Install and run an application that requires a complicated stack of dependencies with a few keystrokes.</li>
<li>Create a pipeline or complex workflow where each individual program is meant to run on a different operating system.</li>
</ul>
<h1 id="container-vs.-virtual-machine">Container vs. Virtual Machine</h1>
<center>
<img src="figures/vm-sif.png" width="70%">
</center>
<h1 id="vm-services">VM services</h1>
<center>
<img src="figures/vm.png" width="70%">
</center>
<p><a href="https://commons.lbl.gov/display/itfaq/SVM+-+Virtual+Machine+Hosting">More information about VM</a></p>
<h1 id="singularity-technology">Singularity Technology</h1>
<ul>
<li>Open-source computer software that encapsulates an application and all its dependencies into a single image</li>
<li>Bring containers and reproducibility to scientific computing and HPC</li>
<li>Developed by Greg Kurtzer</li>
<li>Singularity assumes that you will have a build system where you are the root user, but that you will also have a production system where you may or may not be the root user.</li>
</ul>
<h1 id="singularity-workflow">Singularity Workflow</h1>
<ul>
<li>Install Singularity on a local machine</li>
<li>Build Singularity images/containers on the local machine</li>
<li>Transfer images/containers to LRC clusters</li>
<li>Run images /containers on the cluster
<ul>
<li>Root privilege is not permitted</li>
</ul></li>
</ul>
<h1 id="install-singularity">Install Singularity</h1>
<p>Three OS platforms: - Linux - Mac - Window Installation <a href="https://github.com/lbnl-science-it/container-101/blob/master/singularity_installation_guide.md">instructions</a></p>
<pre><code>$ singularity --version 
$ singularity run docker://godlovedc/lolcow</code></pre>
<h1 id="create-singularity-containers">Create Singularity Containers</h1>
<ul>
<li>Directly from existing containers
<ul>
<li><a href="https://hub.docker.com/search?q=&amp;type=image">Docker hub</a></li>
<li><a href="https://cloud.sylabs.io/library">Sylabs Cloud</a> and <a href="https://singularity-hub.org/">Singularity hub</a></li>
<li><a href="https://www.nvidia.com/en-us/gpu-cloud/containers/">Nvidia HPC containers</a></li>
<li><a href="https://biocontainers.pro/#/registry">Biocontainers</a></li>
<li><a href="https://aws.amazon.com/releasenotes/available-deep-learning-containers-images/">AWS</a></li>
</ul></li>
<li>Build from definition files or recipes</li>
</ul>
<h1 id="singularity-pull">Singularity pull</h1>
<ul>
<li>No root/sudo privilege is needed</li>
<li>Create immutable squashfs containers</li>
</ul>
<pre><code>singularity pull --help</code></pre>
<ul>
<li>Docker Hub:  Pull a container from Docker Hub.</li>
</ul>
<pre><code>singularity pull docker://ubuntu:18.04 
singularity pull docker://gcc:7.2.0</code></pre>
<ul>
<li>Singularity Hub:  If no tag is specified, the master branch of the repository is used</li>
</ul>
<pre><code>singularity pull shub://singularityhub/hello-world</code></pre>
<h1 id="singularity-shell-run-exec">Singularity shell, run, exec</h1>
<ul>
<li><strong>shell</strong> sub-command: invokes an interactive shell within a container</li>
</ul>
<pre><code>singularity shell hello-world_latest.sif</code></pre>
<ul>
<li><strong>run</strong> sub-command: executes the container’s runscript</li>
</ul>
<pre><code>singularity run hello-world_latest.sif </code></pre>
<ul>
<li><strong>exec</strong> sub-command: execute an arbitrary command within container</li>
</ul>
<pre><code>singularity exec hello-world_latest.sif cat /etc/os-release</code></pre>
<h1 id="singularity-build">Singularity build</h1>
<ul>
<li>Root/sudo privilege is needed</li>
</ul>
<pre><code>singularity build --help</code></pre>
<ul>
<li>Build from a definition file</li>
</ul>
<pre><code>sudo singularity --debug build mycontainer.sif Singularity </code></pre>
<h1 id="defination-filerecipe">Defination File/Recipe</h1>
<pre><code>Bootstrap: docker
From: ubuntu
# used singularity run-help 
%help
Hello. I&#39;m in the container.
# executed on host after the base OS is installed.
%setup
    touch ${SINGULARITY_ROOTFS}/tacos.txt
    echo “I love avocado” &gt;&gt; avocados.txt

# copy files from your host system into the container 
%files
    avocados.txt /opt    

%environment
  export NAME=avocado

# executed within the container after the base OS is installed at build time
#install new software and libraries, config files,  directories, etc
%post
    echo &#39;export Avocado=TRUE &gt;&gt; $SINGULARITY_ENVIRONMENT
# executed when the container image is run:  singularity run
%runscript 
    echo &quot;Hello! Arguments received: $* \n&quot;
     exec echo &quot;$@&quot;  </code></pre>
<h1 id="bootstrap-agents">Bootstrap Agents</h1>
<ul>
<li>library (images hosted on the Container Library)</li>
<li>docker (images hosted on Docker Hub)</li>
<li>shub (images hosted on Singularity Hub)</li>
<li>localimage (images saved on your machine)</li>
<li>yum (yum based systems such as CentOS and Scientific Linux)</li>
<li>debootstrap (apt based systems such as Debian and Ubuntu)</li>
<li>arch (Arch Linux)</li>
<li>busybox (BusyBox)</li>
<li>zypper (zypper ba</li>
</ul>
<h1 id="singularity-build-rewritable-sandbox">Singularity Build Rewritable Sandbox</h1>
<ul>
<li>Can be built from a recipe or existing container</li>
<li>Used to develop, test, and make changes, then build or convert it into a standard image</li>
</ul>
<pre><code>sudo singularity build --sandbox build gccbox docker://gcc:7.2.0
sudo singularity build --sandbox build test-box Singularity </code></pre>
<ul>
<li>When you want to alter your image, you can use commands like shell, exec, run, with the –writable option</li>
</ul>
<pre><code>sudo singularity shell --writable test-box</code></pre>
<ul>
<li>Convert a sandbox to an immutable final container:</li>
</ul>
<pre><code>sudo singularity build test-box.sif test-box</code></pre>
<h1 id="inspect-containers">Inspect containers</h1>
<ul>
<li>To check how a images is built, running script and environment variables..</li>
</ul>
<pre><code>singularity inspect [options] image_name
    --lablels
    --runscript
    --deffile
    --environment
e.g. singularity inspect --deffile mycontainer.sif</code></pre>
<h1 id="singularity-python-spython">Singularity Python (spython)</h1>
<ul>
<li>Python API for Singularity containers</li>
<li>Convert Dockerfile to Singularity def</li>
</ul>
<pre><code>spython recipe Dockerfile &gt; dock2sif.def</code></pre>
<h1 id="run-singularity-containers-on-lawrencium">Run Singularity Containers on Lawrencium</h1>
<ul>
<li>File transfer to LRC cluster</li>
</ul>
<pre><code>scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster </code></pre>
<ul>
<li>Run your container interactively
<ul>
<li>Request an interactive compute node</li>
</ul>
<pre><code>singularity shell/run/exec container.sif</code></pre></li>
<li>Submit a slurm job</li>
</ul>
<h1 id="job-submission-example">Job Submission Example</h1>
<pre><code>#!/bin/bash -l
#SBATCH --job-name=container-test        
#SBATCH --partition=lr5          
#SBATCH --account=ac_xxx         
#SBATCH --qos=lr_normal         
#SBATCH --nodes=1           
#SBATCH --time=1-2:0:0          

cd $SLURM_SUBMIT_DIR
singularity exec mycontainer.sif cat /etc/os-release</code></pre>
<h1 id="container-bind-path">Container bind path</h1>
<ul>
<li>Singularity allow mapping directories on host to directories within container</li>
<li>Easy data access within containers</li>
<li>System-defined bind paths on LRC
<ul>
<li>/global/home/users/</li>
<li>/globa/scratch/</li>
</ul></li>
<li>User can define own bind paths:</li>
<li>e.g.: mount /host/path/ on the host to /container/path inside the container</li>
</ul>
<pre><code>-B /host/path/:/container/path
singularity shell --nv -B /clusterfs/bear:/tmp pytorch_19_12_py3.sif</code></pre>
<h1 id="run-gpu-containers">Run GPU Containers</h1>
<ul>
<li>Singularity supports NVIDIA’s CUDA GPU compute framework or AMD’s ROCm solution</li>
<li>–nv enables NVIDIA GPU support in Singularity</li>
<li>Remember to request a GPU node from the ES1 partition</li>
</ul>
<pre><code>singularity exec --nv pytorch_19_12_py3.sif python -c &quot;import torch; print(torch.__version__)&quot;
1.4.0a0+a5b4d78</code></pre>
<h1 id="getting-help">Getting help</h1>
<ul>
<li>Virtual Office Hours:
<ul>
<li>Time: 10:30am - noon (Wednesdays)</li>
<li>Request online</li>
</ul></li>
<li>Sending us tickets at hpcshelp@lbl.gov</li>
<li>More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/</li>
<li>DLab consulting: https://dlab.berkeley.edu/consulting</li>
</ul>
</body>
</html>
