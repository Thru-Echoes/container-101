<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="June 10, 2020" />
  <title>Container 101 on Lawrencium</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Container 101 on Lawrencium</h1>
<p class="author">June 10, 2020</p>
<p class="date">Wei Feinstein</p>
</header>
<h1 id="outline">Outline</h1>
<ul>
<li>Lawrencium Supercluster overview</li>
<li>Computing resources &amp; services<br />
</li>
<li>Container technology overview</li>
<li>Build singularity containers</li>
<li>Run singularity containers on Lawrencium</li>
</ul>
<h1 id="lawrencium-cluster-overview">Lawrencium Cluster Overview</h1>
<center>
<img src="figures/lrc.png" width="80%">
</center>
<h1 id="lawrencium-condo-cluster">Lawrencium Condo Cluster</h1>
<ul>
<li>Lawrencium is a LBNL Condo Computer clusterresources
<ul>
<li>LBNL investmeent</li>
<li>Individual PIs purchase nodes and storage</li>
<li>Computational cycles are shared among all lawrencium users</li>
</ul></li>
<li>Cluster infrastructure:
<ul>
<li>OTP authentication</li>
<li>High speed infiniBand parallel file system for fast inter-node communication</li>
<li>OS and security updates, software module farm, job scheduler SLURM</li>
<li>Home/project storage, lustre parallel file system and backend network infrastructure.</li>
</ul></li>
</ul>
<h1 id="accounts-on-lawrencium">Accounts on Lawrencium</h1>
<h2 id="three-types-of-project-accounts">Three types of Project Accounts</h2>
<ul>
<li><p>PI Computing Allowance (PCA) account: free 300K SUs per year (pc_xxx)</p></li>
<li><p>Condo account:</p>
<ul>
<li>PIs purchase and contribute compute nodes to the general condo pool (lr_xxx)</li>
<li>Jobs are free of charge and have highest priority</li>
</ul></li>
<li><p>Recharge account: with minimal recharge rate ~ $0.01/SU (ac_xxx)</p></li>
<li><p>$25 per user per month</p></li>
</ul>
<p>https://sites.google.com/a/lbl.gov/hpc/getting-an-account</p>
<h2 id="user-accounts">User accounts</h2>
<ul>
<li>User account request</li>
<li>User agreement consent</li>
</ul>
<p>https://sites.google.com/a/lbl.gov/hpc/getting-an-account</p>
<h1 id="softwre-module-farm">Softwre Module Farm</h1>
<center>
<img src="figures/SMF.png" width="60%">
</center>
<h1 id="module-commands">Module commands</h1>
<ul>
<li><em>module purge</em>: clear user’s work environment</li>
<li><em>module avail</em>: check available software packages</li>
<li><em>module load xxx</em>: load a package</li>
<li><em>module list</em>: check currently loaded software</li>
<li>Users may install their own software</li>
</ul>
<p>https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/sl6-module-farm-guide</p>
<h1 id="slurm">SLURM:</h1>
<h2 id="jub-submission">Jub Submission</h2>
<ul>
<li>sbatch: submit a job to the batch queue system</li>
</ul>
<pre><code>sbatch myjob.sh</code></pre>
<ul>
<li>srun: request an interactive node(s) and login automatically</li>
</ul>
<pre><code>srun -A ac_xxx -p lr5 -q lr_normal -t 1:0:0 --pty bash</code></pre>
<ul>
<li>salloc : request an interactive node(s)</li>
</ul>
<pre><code>salloc –A pc_xxx –p lr6 –q lr_debug –t 0:30:0</code></pre>
<h2 id="job-monitoring">Job Monitoring</h2>
<ul>
<li>sinfo: check status of partitions and nodes (idle, allocated, drain, down)</li>
</ul>
<pre><code>sinfo –r –p lr6</code></pre>
<ul>
<li>squeue: check jobs in the batch queuing system (R or PD)</li>
</ul>
<pre><code>squeue –u $USER</code></pre>
<ul>
<li>sacct: check job information or history</li>
</ul>
<pre><code>sacct -X -o ‘jobid,user,partition,nodelist,stat’</code></pre>
<ul>
<li>scancel : cancel a job</li>
</ul>
<pre><code>scancel jobID</code></pre>
<p>https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/scheduler/slurm-usage-instructions</p>
<h1 id="services-1">Services (1)</h1>
<ul>
<li>Designate Data Transfer node <strong>lrc-xfer.lbl.gov</strong>
<ul>
<li>scp -r /your/source/file $USER@lrc-xder.lbl.gov:/cluster/path</li>
<li>rsync -avzh /your/source/file $USER <span class="citation" data-cites="lrc-xfer.lbl.gov:/cluster/path">@lrc-xfer.lbl.gov:/cluster/path</span></li>
</ul></li>
<li>Globus Online provide secured unified interface for data transfer
<ul>
<li>endpoint lbn#lrc, Globus Connect, AWS S3 connect</li>
</ul></li>
<li>Visualization and remote desktop
<ul>
<li>Detailed information <a href="https://sites.google.com/a/lbl.gov/high-performance-computing-services-group/getting-started/remote-desktop">click here</a></li>
</ul></li>
</ul>
<h1 id="service-2-jupyterhub">Service (2): Jupyterhub</h1>
<p>https://lrc-jupyter.lbl.gov</p>
<center>
<img src="figures/jupyter.png" width="80%">
</center>
<h1 id="services-3-cloud-computing">Services (3): Cloud Computing</h1>
<ul>
<li>LBNL has a master payer program for cloud services on Amazon Web Services (AWS) and Google Cloud Platform (GCP).<br />
</li>
<li>No charge to have an account in the program</li>
<li>Charges only for actual usage of cloud services like storage or compute</li>
<li>Monthly billing for cloud usage are direct to your PID via recharge mechanism</li>
<li>Simple enrollment process.</li>
<li>De-enrollment only changes the billing setup in your account, and your account will continue to be active.</li>
<li>Complete control of your own AWS or GCP dashboard, your data, tools, and services.</li>
</ul>
<h1 id="servicesv-4-cloud-computing">Servicesv (4): Cloud Computing</h1>
<ul>
<li>AWS and GCP make discounts available to LBNL users.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Type</th>
<th>AWS</th>
<th>GCP</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Overall</td>
<td>7%</td>
<td>13%</td>
<td></td>
</tr>
<tr class="even">
<td>Data Egress</td>
<td>15%</td>
<td>25%</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>AWS, <a href="https://aws.amazon.com/blogs/publicsector/aws-offers-data-egress-discount-to-researchers/">restrictions apply</a> |</li>
<li>Over 125 people at LBNL with cloud accounts on AWS and GCP.<br />
</li>
<li>Mostly use virtual machines and storage</li>
<li>Use containerization, machine learning, AI, and data visualization services on both platforms</li>
<li>To set up a cloud account on either AWS or GCP, send email to <a href="mailto:scienceit@lbl.gov">scienceit@lbl.gov</a></li>
</ul>
<h1 id="containerization">Containerization</h1>
<ul>
<li>Technology of putting an application and all of its dependencies into a single package.</li>
<li>Portable, shareable, and reproducible.</li>
<li>Your application brings its environment with it.</li>
</ul>
<h1 id="containerization-examples">Containerization Examples</h1>
<ul>
<li>Package an analysis pipeline so that it runs on your laptop, in the cloud, and in a high performance computing (HPC) environment to produce the same result.</li>
<li>Publish a paper and include a link to a container with all of the data and software that you used so that others can easily reproduce your results.</li>
<li>Install and run an application that requires a complicated stack of dependencies with a few keystrokes.</li>
<li>Create a pipeline or complex workflow where each individual program is meant to run on a different operating system.</li>
</ul>
<h1 id="container-vs.-virtual-machine">Container vs. Virtual Machine</h1>
<center>
<img src="figures/vm-sif.png" width="70%">
</center>
<h1 id="vm-services">VM services</h1>
<center>
<img src="figures/vm.png" width="50%">
</center>
<p><a href="https://commons.lbl.gov/display/itfaq/SVM+-+Virtual+Machine+Hosting">More information about VM</a></p>
<h1 id="singularity-technology">Singularity Technology</h1>
<ul>
<li>Open-source computer software that encapsulates an application and all its dependencies into a single image</li>
<li>Bring containers and reproducibility to scientific computing and HPC</li>
<li>Developed by Greg Kurtzer</li>
<li>Singularity assumes that you will have a build system where you are the root user, but that you will also have a production system where you may or may not be the root user.</li>
</ul>
<h1 id="singularity-workflow">Singularity Workflow</h1>
<ul>
<li>Install Singularity on a local machine</li>
<li>Build Singularity images/containers on the local machine</li>
<li>Transfer images/containers to LRC clusters</li>
<li>Run images /containers on the cluster
<ul>
<li>Root privilege is not permitted</li>
</ul></li>
</ul>
<h1 id="install-singularity">Install Singularity</h1>
<p>Three OS platforms: - Linux - Mac - Window Installation <a href="https://github.com/lbnl-science-it/container-101/blob/master/singularity_installation_guide.md">instructions</a></p>
<pre><code>$ singularity --version 
$ singularity run docker://godlovedc/lolcow</code></pre>
<h1 id="create-singularity-containers">Create Singularity Containers</h1>
<ul>
<li>Directly from existing containers
<ul>
<li><a href="https://hub.docker.com/search?q=&amp;type=image">Docker hub</a></li>
<li><a href="https://cloud.sylabs.io/library">Sylabs Cloud</a> and <a href="https://singularity-hub.org/">Singularity hub</a></li>
<li><a href="https://www.nvidia.com/en-us/gpu-cloud/containers/">Nvidia HPC containers</a></li>
<li><a href="https://biocontainers.pro/#/registry">Biocontainers</a></li>
<li><a href="https://aws.amazon.com/releasenotes/available-deep-learning-containers-images/">AWS</a></li>
</ul></li>
<li>Build from definition files or recipes</li>
</ul>
<h1 id="singularity-pull">Singularity pull</h1>
<ul>
<li>No root/sudo privilege is needed</li>
<li>Create immutable squashfs containers</li>
</ul>
<pre><code>singularity pull --help</code></pre>
<ul>
<li>Docker Hub:  Pull a container from Docker Hub.</li>
</ul>
<pre><code>singularity pull docker://ubuntu:18.04 
singularity pull docker://gcc:7.2.0</code></pre>
<ul>
<li>Singularity Hub:  If no tag is specified, the master branch of the repository is used</li>
</ul>
<pre><code>singularity pull shub://singularityhub/hello-world</code></pre>
<h1 id="singularity-shell-run-exec">Singularity shell, run, exec</h1>
<ul>
<li><strong>shell</strong> sub-command: invokes an interactive shell within a container</li>
</ul>
<pre><code>singularity shell hello-world_latest.sif</code></pre>
<ul>
<li><strong>run</strong> sub-command: executes the container’s runscript</li>
</ul>
<pre><code>singularity run hello-world_latest.sif </code></pre>
<ul>
<li><strong>exec</strong> sub-command: execute an arbitrary command within container</li>
</ul>
<pre><code>singularity exec hello-world_latest.sif cat /etc/os-release</code></pre>
<h1 id="singularity-build">Singularity build</h1>
<ul>
<li>Root/sudo privilege is needed</li>
</ul>
<pre><code>singularity build --help</code></pre>
<ul>
<li>Build from a definition file</li>
</ul>
<pre><code>sudo singularity --debug build mycontainer.sif Singularity </code></pre>
<h1 id="defination-filerecipe">Defination File/Recipe</h1>
<pre><code>Bootstrap: docker
From: ubuntu
# used singularity run-help 
%help
Hello. I&#39;m in the container.
# executed on host after the base OS is installed.
%setup
    touch ${SINGULARITY_ROOTFS}/tacos.txt
    echo “I love avocado” &gt;&gt; avocados.txt

# copy files from your host system into the container 
%files
    avocados.txt /opt    

%environment
  export NAME=avocado

# executed within the container after the base OS is installed at build time
#install new software and libraries, config files,  directories, etc
%post
    echo &#39;export Avocado=TRUE &gt;&gt; $SINGULARITY_ENVIRONMENT
# executed when the container image is run:  singularity run
%runscript 
    echo &quot;Hello! Arguments received: $* \n&quot;
     exec echo &quot;$@&quot;  </code></pre>
<h1 id="bootstrap-agents">Bootstrap Agents</h1>
<ul>
<li>library (images hosted on the Container Library)</li>
<li>docker (images hosted on Docker Hub)</li>
<li>shub (images hosted on Singularity Hub)</li>
<li>localimage (images saved on your machine)</li>
<li>yum (yum based systems such as CentOS and Scientific Linux)</li>
<li>debootstrap (apt based systems such as Debian and Ubuntu)</li>
<li>arch (Arch Linux)</li>
<li>busybox (BusyBox)</li>
<li>zypper (zypper ba</li>
</ul>
<h1 id="singularity-build-rewritable-sandbox">Singularity Build Rewritable Sandbox</h1>
<ul>
<li>Can be built from a recipe or existing container</li>
<li>Used to develop, test, and make changes, then build or convert it into a standard image</li>
</ul>
<pre><code>sudo singularity build --sandbox build gccbox docker://gcc:7.2.0
sudo singularity build --sandbox build test-box Singularity </code></pre>
<ul>
<li>When you want to alter your image, you can use commands like shell, exec, run, with the –writable option</li>
</ul>
<pre><code>sudo singularity shell --writable test-box</code></pre>
<ul>
<li>Convert a sandbox to an immutable final container:</li>
</ul>
<pre><code>sudo singularity build test-box.sif test-box</code></pre>
<h1 id="inspect-containers">Inspect containers</h1>
<ul>
<li>To check how a images is built, running script and environment variables..</li>
</ul>
<pre><code>singularity inspect [options] image_name
    --lablels
    --runscript
    --deffile
    --environment
e.g. singularity inspect --deffile mycontainer.sif</code></pre>
<h1 id="singularity-python-spython">Singularity Python (spython)</h1>
<ul>
<li>Python API for Singularity containers</li>
<li>Convert Dockerfile to Singularity def</li>
</ul>
<pre><code>spython recipe Dockerfile &gt; dock2sif.def</code></pre>
<h1 id="run-singularity-containers-on-lawrencium">Run Singularity Containers on Lawrencium</h1>
<ul>
<li>File transfer to LRC cluster</li>
</ul>
<pre><code>scp xxx.sif $USER@lrc-xfer.lbl.gov:/your/path/on/cluster </code></pre>
<ul>
<li>Run your container interactively
<ul>
<li>Request an interactive compute node</li>
</ul>
<pre><code>singularity shell/run/exec container.sif</code></pre></li>
<li>Submit a slurm job</li>
</ul>
<h1 id="job-submission-example">Job Submission Example</h1>
<pre><code>#!/bin/bash -l
#SBATCH --job-name=container-test        
#SBATCH --partition=lr5          
#SBATCH --account=ac_xxx         
#SBATCH --qos=lr_normal         
#SBATCH --nodes=1           
#SBATCH --time=1-2:0:0          

cd $SLURM_SUBMIT_DIR
singularity exec mycontainer.sif cat /etc/os-release</code></pre>
<h1 id="container-bind-path">Container bind path</h1>
<ul>
<li>Singularity allow mapping directories on host to directories within container</li>
<li>Easy data access within containers</li>
<li>System-defined bind paths on LRC
<ul>
<li>/global/home/users/</li>
<li>/globa/scratch/</li>
</ul></li>
<li>User can define own bind paths:</li>
<li>e.g.: mount /host/path/ on the host to /container/path inside the container</li>
</ul>
<pre><code>-B /host/path/:/container/path
singularity shell --nv -B /clusterfs/bear:/tmp pytorch_19_12_py3.sif</code></pre>
<h1 id="run-gpu-containers">Run GPU Containers</h1>
<ul>
<li>Singularity supports NVIDIA’s CUDA GPU compute framework or AMD’s ROCm solution</li>
<li>–nv enables NVIDIA GPU support in Singularity</li>
<li>Remember to request a GPU node from the ES1 partition</li>
</ul>
<pre><code>singularity exec --nv pytorch_19_12_py3.sif python -c &quot;import torch; print(torch.__version__)&quot;
1.4.0a0+a5b4d78</code></pre>
<h1 id="getting-help">Getting help</h1>
<ul>
<li>Virtual Office Hours:
<ul>
<li>Time: 10:30am - noon (Wednesdays)</li>
<li>Request online</li>
</ul></li>
<li>Sending us tickets at hpcshelp@lbl.gov</li>
<li>More information, documents, tips of how to use LBNL Supercluster http://scs.lbl.gov/</li>
<li>DLab consulting: https://dlab.berkeley.edu/consulting</li>
</ul>
</body>
</html>
